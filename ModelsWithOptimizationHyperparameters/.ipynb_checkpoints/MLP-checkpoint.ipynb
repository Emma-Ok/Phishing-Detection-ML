{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a16935e-8d6f-4d4e-a536-eb89719f49a6",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/Emma-Ok/Phishing-Detection-ML/blob/alc4dev/ModelsWithAllSamples/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63470f41-ecd8-4a06-87f7-39f1976e959c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 23:51:16.295645: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-27 23:51:16.299615: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-06-27 23:51:16.311948: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751068276.333391     287 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751068276.339521     287 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751068276.356358     287 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751068276.356383     287 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751068276.356386     287 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751068276.356388     287 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-27 23:51:16.362366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (10000, 49)\n",
      "Distribución de clases:\n",
      "CLASS_LABEL\n",
      "1    5000\n",
      "0    5000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import (classification_report, roc_auc_score, confusion_matrix, \n",
    "                           ConfusionMatrixDisplay, roc_curve, auc, f1_score, accuracy_score,\n",
    "                           precision_score, recall_score, matthews_corrcoef, precision_recall_curve)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de semilla para reproducibilidad\n",
    "RANDOM_STATE = 42\n",
    "tf.random.set_seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Carga directa desde GitHub\n",
    "url = \"https://raw.githubusercontent.com/Emma-Ok/Phishing-Detection-ML/main/Phishing_Legitimate_full.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df = df.drop(columns=['id'])\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Distribución de clases:\\n{df['CLASS_LABEL'].value_counts()}\")\n",
    "\n",
    "# Separar X e y\n",
    "X = df.drop(columns=['CLASS_LABEL'])\n",
    "y = df['CLASS_LABEL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fcd296a-5bea-4686-8b86-cf7a7c657f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhishingNeuralNetwork:\n",
    "    \"\"\"\n",
    "    Clase mejorada para implementar red neuronal MLP para detección de phishing\n",
    "    optimizada para datasets balanceados con mejores prácticas de ML.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, random_state: int = 42, validation_split: float = 0.2):\n",
    "        self.random_state = random_state\n",
    "        self.validation_split = validation_split\n",
    "\n",
    "        np.random.seed(random_state)\n",
    "        tf.random.set_seed(random_state)\n",
    "\n",
    "        self.scaler = RobustScaler()\n",
    "        self.alternative_scaler = StandardScaler()\n",
    "\n",
    "        self.best_model = None\n",
    "        self.tuner = None\n",
    "        self.history = None\n",
    "        self.cv_scores = None\n",
    "\n",
    "        self.input_dim = None\n",
    "        self.best_params = None\n",
    "\n",
    "    def build_model(self, hp):\n",
    "        model = keras.Sequential()\n",
    "        model.add(layers.InputLayer(input_shape=(self.input_dim,)))\n",
    "\n",
    "        if hp.Boolean('use_batch_norm'):\n",
    "            model.add(layers.BatchNormalization())\n",
    "\n",
    "        activation = hp.Choice('activation', values=['relu', 'tanh', 'sigmoid', 'elu', 'swish'])\n",
    "        reg_type = hp.Choice('regularization_type', values=['l1', 'l2', 'l1_l2'])\n",
    "        reg_strength = hp.Float('reg_strength', min_value=1e-5, max_value=1e-1, sampling='LOG')\n",
    "\n",
    "        if reg_type == 'l1':\n",
    "            regularizer = keras.regularizers.l1(reg_strength)\n",
    "        elif reg_type == 'l2':\n",
    "            regularizer = keras.regularizers.l2(reg_strength)\n",
    "        else:\n",
    "            regularizer = keras.regularizers.l1_l2(l1=reg_strength/2, l2=reg_strength/2)\n",
    "\n",
    "        n_layers = hp.Int('n_hidden_layers', min_value=1, max_value=4)\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:\n",
    "                units = hp.Int(f'units_layer_{i}', min_value=32, max_value=512, step=32)\n",
    "            else:\n",
    "                prev_units = hp.get(f'units_layer_{i-1}')\n",
    "                max_units = max(32, prev_units)\n",
    "                units = hp.Int(f'units_layer_{i}', min_value=16, max_value=max_units, step=16)\n",
    "\n",
    "            model.add(layers.Dense(\n",
    "                units,\n",
    "                activation=activation,\n",
    "                kernel_regularizer=regularizer,\n",
    "                kernel_initializer='he_normal',\n",
    "                name=f'hidden_{i+1}'\n",
    "            ))\n",
    "\n",
    "            dropout_rate = hp.Float(f'dropout_{i}', min_value=0.0, max_value=0.6, step=0.1)\n",
    "            if dropout_rate > 0:\n",
    "                model.add(layers.Dropout(dropout_rate))\n",
    "\n",
    "        model.add(layers.Dense(1, activation='sigmoid', name='output'))\n",
    "\n",
    "        optimizer_type = hp.Choice('optimizer', values=['adam', 'adamw', 'rmsprop'])\n",
    "        learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-1, sampling='LOG')\n",
    "\n",
    "        if optimizer_type == 'adam':\n",
    "            optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        elif optimizer_type == 'adamw':\n",
    "            optimizer = keras.optimizers.AdamW(learning_rate=learning_rate, weight_decay=1e-4)\n",
    "        else:\n",
    "            optimizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=[\n",
    "                'accuracy',\n",
    "                keras.metrics.Precision(name='precision'),\n",
    "                keras.metrics.Recall(name='recall'),\n",
    "                keras.metrics.F1Score(name='f1_score')\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return model\n",
    "\n",
    "    def prepare_data(self, X, y, test_size=0.2, use_robust_scaling=True):\n",
    "        print(f\"Dataset original: {X.shape[0]} muestras, {X.shape[1]} características\")\n",
    "        print(f\"Distribución de clases: {np.bincount(y)}\")\n",
    "\n",
    "        class_ratio = min(np.bincount(y)) / max(np.bincount(y))\n",
    "        if class_ratio < 0.8:\n",
    "            print(f\"⚠️  Dataset poco balanceado (ratio: {class_ratio:.3f})\")\n",
    "        else:\n",
    "            print(f\"✅ Dataset bien balanceado (ratio: {class_ratio:.3f})\")\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state, stratify=y\n",
    "        )\n",
    "\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X_train, y_train, test_size=self.validation_split,\n",
    "            random_state=self.random_state, stratify=y_train\n",
    "        )\n",
    "\n",
    "        scaler = self.scaler if use_robust_scaling else self.alternative_scaler\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_val_scaled = scaler.transform(X_val)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        print(f\"\\nDistribución final:\")\n",
    "        print(f\"Entrenamiento: {X_train_scaled.shape[0]} muestras\")\n",
    "        print(f\"Validación: {X_val_scaled.shape[0]} muestras\")\n",
    "        print(f\"Prueba: {X_test_scaled.shape[0]} muestras\")\n",
    "\n",
    "        return X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "    def optimize_hyperparameters(self, X_train, y_train, X_val, y_val,\n",
    "                                 max_trials=100, epochs=100):\n",
    "        print(f\"Modo estándar: Optimización con split fijo ({max_trials} trials)\")\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        self.tuner = kt.Hyperband(\n",
    "            self.build_model,\n",
    "            objective=kt.Objective('val_f1_score', direction='max'),\n",
    "            max_epochs=epochs,\n",
    "            factor=3,\n",
    "            directory='phishing_tuning_improved',\n",
    "            project_name='neural_network_phishing_v2',\n",
    "            seed=self.random_state\n",
    "        )\n",
    "\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(monitor='val_f1_score', patience=15,\n",
    "                                          restore_best_weights=True, mode='max'),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7,\n",
    "                                              patience=7, min_lr=1e-6, verbose=1),\n",
    "            keras.callbacks.TerminateOnNaN()\n",
    "        ]\n",
    "\n",
    "        self.tuner.search(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        best_hps = self.tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        self.best_params = best_hps.values\n",
    "\n",
    "        print(\"\\nMejores hiperparámetros:\")\n",
    "        for param, value in self.best_params.items():\n",
    "            print(f\"  {param}: {value}\")\n",
    "\n",
    "        return best_hps\n",
    "\n",
    "    def train_final_model(self, X_train, y_train, X_val, y_val, best_hps, epochs=200):\n",
    "        print(\"Entrenando modelo final...\")\n",
    "        self.best_model = self.tuner.hypermodel.build(best_hps)\n",
    "\n",
    "        callbacks = [\n",
    "            keras.callbacks.EarlyStopping(monitor='val_f1_score', patience=25,\n",
    "                                          restore_best_weights=True, mode='max'),\n",
    "            keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.8,\n",
    "                                              patience=10, min_lr=1e-7),\n",
    "            keras.callbacks.ModelCheckpoint('best_phishing_model.h5',\n",
    "                                            monitor='val_f1_score',\n",
    "                                            save_best_only=True, mode='max')\n",
    "        ]\n",
    "\n",
    "        self.history = self.best_model.fit(\n",
    "            X_train, y_train,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val, y_val),\n",
    "            callbacks=callbacks,\n",
    "            batch_size=32,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def evaluate_comprehensive(self, X_test, y_test):\n",
    "        if self.best_model is None:\n",
    "            raise ValueError(\"Modelo no entrenado.\")\n",
    "\n",
    "        y_pred_proba = self.best_model.predict(X_test, verbose=0).flatten()\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        auc_roc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        mcc = matthews_corrcoef(y_test, y_pred)\n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        geometric_mean = np.sqrt(sensitivity * specificity)\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "\n",
    "        metrics = {\n",
    "            'accuracy': accuracy,\n",
    "            'balanced_accuracy': balanced_accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'sensitivity': sensitivity,\n",
    "            'specificity': specificity,\n",
    "            'f1_score': f1,\n",
    "            'geometric_mean': geometric_mean,\n",
    "            'auc_roc': auc_roc,\n",
    "            'mcc': mcc,\n",
    "            'confusion_matrix': cm,\n",
    "            'classification_report': classification_report(y_test, y_pred),\n",
    "            'roc_curve': (fpr, tpr),\n",
    "            'pr_curve': (precision_curve, recall_curve)\n",
    "        }\n",
    "\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"EVALUACIÓN FINAL DEL MODELO\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Balanced Accuracy: {balanced_accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "        print(f\"Specificity: {specificity:.4f}\")\n",
    "        print(f\"F1-Score: {f1:.4f}\")\n",
    "        print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "        print(f\"MCC: {mcc:.4f}\")\n",
    "        print(f\"Geometric Mean: {geometric_mean:.4f}\")\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def plot_comprehensive_results(self, metrics):\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "        if self.history:\n",
    "            ax = axes[0, 0]\n",
    "            ax.plot(self.history.history['loss'], label='Train Loss', alpha=0.8)\n",
    "            ax.plot(self.history.history['val_loss'], label='Val Loss', alpha=0.8)\n",
    "            ax.set_title('Training History - Loss')\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel('Loss')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        if self.history:\n",
    "            ax = axes[0, 1]\n",
    "            ax.plot(self.history.history['f1_score'], label='Train F1', alpha=0.8)\n",
    "            ax.plot(self.history.history['val_f1_score'], label='Val F1', alpha=0.8)\n",
    "            ax.set_title('Training History - F1 Score')\n",
    "            ax.set_xlabel('Epoch')\n",
    "            ax.set_ylabel('F1 Score')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "        ax = axes[0, 2]\n",
    "        sns.heatmap(metrics['confusion_matrix'], annot=True, fmt='d',\n",
    "                    cmap='Blues', ax=ax, cbar_kws={'shrink': 0.8})\n",
    "        ax.set_title('Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('Actual')\n",
    "\n",
    "        ax = axes[1, 0]\n",
    "        fpr, tpr = metrics['roc_curve']\n",
    "        ax.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {metrics[\"auc_roc\"]:.3f})')\n",
    "        ax.plot([0, 1], [0, 1], 'k--', alpha=0.5)\n",
    "        ax.set_xlabel('False Positive Rate')\n",
    "        ax.set_ylabel('True Positive Rate')\n",
    "        ax.set_title('ROC Curve')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        ax = axes[1, 1]\n",
    "        precision_curve, recall_curve = metrics['pr_curve']\n",
    "        ax.plot(recall_curve, precision_curve, linewidth=2, label='PR Curve')\n",
    "        ax.set_xlabel('Recall')\n",
    "        ax.set_ylabel('Precision')\n",
    "        ax.set_title('Precision-Recall Curve')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        ax = axes[1, 2]\n",
    "        metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'AUC-ROC', 'MCC']\n",
    "        metric_values = [\n",
    "            metrics['accuracy'], metrics['precision'], metrics['recall'],\n",
    "            metrics['f1_score'], metrics['auc_roc'], metrics['mcc']\n",
    "        ]\n",
    "\n",
    "        bars = ax.barh(metric_names, metric_values, color='skyblue', alpha=0.7)\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_title('Performance Metrics Summary')\n",
    "\n",
    "        for bar, value in zip(bars, metric_values):\n",
    "            ax.text(value + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "                    f'{value:.3f}', va='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75bead97-ccd3-4589-8c1c-0b846e2e2f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset original: 10000 muestras, 48 características\n",
      "Distribución de clases: [5000 5000]\n",
      "✅ Dataset bien balanceado (ratio: 1.000)\n",
      "\n",
      "Distribución final:\n",
      "Entrenamiento: 6400 muestras\n",
      "Validación: 1600 muestras\n",
      "Prueba: 2000 muestras\n",
      "Modo estándar: Optimización con split fijo (20 trials)\n",
      "Reloading Tuner from phishing_tuning_improved/neural_network_phishing_v2/tuner0.json\n",
      "\n",
      "Mejores hiperparámetros:\n",
      "  use_batch_norm: True\n",
      "  activation: relu\n",
      "  regularization_type: l2\n",
      "  reg_strength: 0.00012232117875737863\n",
      "  n_hidden_layers: 4\n",
      "  units_layer_0: 192\n",
      "  dropout_0: 0.30000000000000004\n",
      "  optimizer: adam\n",
      "  learning_rate: 0.0031089211420179527\n",
      "  tuner/epochs: 20\n",
      "  tuner/initial_epoch: 7\n",
      "  tuner/bracket: 2\n",
      "  tuner/round: 2\n",
      "  units_layer_1: 16\n",
      "  dropout_1: 0.0\n",
      "  units_layer_2: 16\n",
      "  dropout_2: 0.0\n",
      "  units_layer_3: 16\n",
      "  dropout_3: 0.0\n",
      "  tuner/trial_id: 0012\n",
      "Entrenando modelo final...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-27 23:51:42.348775: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m196/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8336 - f1_score: 0.6720 - loss: 0.4205 - precision: 0.8294 - recall: 0.8524"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8350 - f1_score: 0.6719 - loss: 0.4181 - precision: 0.8308 - recall: 0.8535 - val_accuracy: 0.9563 - val_f1_score: 0.6667 - val_loss: 0.1877 - val_precision: 0.9679 - val_recall: 0.9438 - learning_rate: 0.0031\n",
      "Epoch 2/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9291 - f1_score: 0.6719 - loss: 0.2346 - precision: 0.9277 - recall: 0.9326 - val_accuracy: 0.9650 - val_f1_score: 0.6667 - val_loss: 0.1516 - val_precision: 0.9721 - val_recall: 0.9575 - learning_rate: 0.0031\n",
      "Epoch 3/20\n",
      "\u001b[1m187/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9456 - f1_score: 0.6722 - loss: 0.2006 - precision: 0.9432 - recall: 0.9497"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9456 - f1_score: 0.6719 - loss: 0.2001 - precision: 0.9431 - recall: 0.9498 - val_accuracy: 0.9606 - val_f1_score: 0.6669 - val_loss: 0.1585 - val_precision: 0.9743 - val_recall: 0.9463 - learning_rate: 0.0031\n",
      "Epoch 4/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9468 - f1_score: 0.6719 - loss: 0.1851 - precision: 0.9478 - recall: 0.9472 - val_accuracy: 0.9700 - val_f1_score: 0.6669 - val_loss: 0.1385 - val_precision: 0.9724 - val_recall: 0.9675 - learning_rate: 0.0031\n",
      "Epoch 5/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9545 - f1_score: 0.6719 - loss: 0.1681 - precision: 0.9525 - recall: 0.9580 - val_accuracy: 0.9712 - val_f1_score: 0.6667 - val_loss: 0.1331 - val_precision: 0.9772 - val_recall: 0.9650 - learning_rate: 0.0031\n",
      "Epoch 6/20\n",
      "\u001b[1m189/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9577 - f1_score: 0.6722 - loss: 0.1544 - precision: 0.9596 - recall: 0.9570"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9578 - f1_score: 0.6719 - loss: 0.1543 - precision: 0.9594 - recall: 0.9573 - val_accuracy: 0.9681 - val_f1_score: 0.6672 - val_loss: 0.1392 - val_precision: 0.9711 - val_recall: 0.9650 - learning_rate: 0.0031\n",
      "Epoch 7/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9653 - f1_score: 0.6721 - loss: 0.1415 - precision: 0.9684 - recall: 0.9629 - val_accuracy: 0.9731 - val_f1_score: 0.6667 - val_loss: 0.1376 - val_precision: 0.9761 - val_recall: 0.9700 - learning_rate: 0.0031\n",
      "Epoch 8/20\n",
      "\u001b[1m196/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9649 - f1_score: 0.6723 - loss: 0.1380 - precision: 0.9663 - recall: 0.9643"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9649 - f1_score: 0.6722 - loss: 0.1379 - precision: 0.9662 - recall: 0.9644 - val_accuracy: 0.9744 - val_f1_score: 0.6675 - val_loss: 0.1237 - val_precision: 0.9738 - val_recall: 0.9750 - learning_rate: 0.0031\n",
      "Epoch 9/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9626 - f1_score: 0.6720 - loss: 0.1372 - precision: 0.9630 - recall: 0.9631 - val_accuracy: 0.9737 - val_f1_score: 0.6667 - val_loss: 0.1249 - val_precision: 0.9749 - val_recall: 0.9725 - learning_rate: 0.0031\n",
      "Epoch 10/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9703 - f1_score: 0.6724 - loss: 0.1223 - precision: 0.9703 - recall: 0.9710"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9703 - f1_score: 0.6724 - loss: 0.1224 - precision: 0.9703 - recall: 0.9710 - val_accuracy: 0.9681 - val_f1_score: 0.6683 - val_loss: 0.1398 - val_precision: 0.9629 - val_recall: 0.9737 - learning_rate: 0.0031\n",
      "Epoch 11/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9724 - f1_score: 0.6731 - loss: 0.1167 - precision: 0.9702 - recall: 0.9754 - val_accuracy: 0.9725 - val_f1_score: 0.6681 - val_loss: 0.1231 - val_precision: 0.9701 - val_recall: 0.9750 - learning_rate: 0.0031\n",
      "Epoch 12/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9724 - f1_score: 0.6734 - loss: 0.1165 - precision: 0.9714 - recall: 0.9741"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9724 - f1_score: 0.6734 - loss: 0.1165 - precision: 0.9713 - recall: 0.9741 - val_accuracy: 0.9731 - val_f1_score: 0.6697 - val_loss: 0.1296 - val_precision: 0.9656 - val_recall: 0.9812 - learning_rate: 0.0031\n",
      "Epoch 13/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9757 - f1_score: 0.6761 - loss: 0.1087 - precision: 0.9769 - recall: 0.9750 - val_accuracy: 0.9700 - val_f1_score: 0.6686 - val_loss: 0.1238 - val_precision: 0.9642 - val_recall: 0.9762 - learning_rate: 0.0031\n",
      "Epoch 14/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9701 - f1_score: 0.6752 - loss: 0.1222 - precision: 0.9662 - recall: 0.9751 - val_accuracy: 0.9700 - val_f1_score: 0.6686 - val_loss: 0.1234 - val_precision: 0.9642 - val_recall: 0.9762 - learning_rate: 0.0031\n",
      "Epoch 15/20\n",
      "\u001b[1m191/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9756 - f1_score: 0.6765 - loss: 0.1063 - precision: 0.9740 - recall: 0.9778"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9753 - f1_score: 0.6762 - loss: 0.1067 - precision: 0.9738 - recall: 0.9775 - val_accuracy: 0.9737 - val_f1_score: 0.6700 - val_loss: 0.1286 - val_precision: 0.9702 - val_recall: 0.9775 - learning_rate: 0.0031\n",
      "Epoch 16/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9764 - f1_score: 0.6813 - loss: 0.1036 - precision: 0.9772 - recall: 0.9761 - val_accuracy: 0.9681 - val_f1_score: 0.6697 - val_loss: 0.1425 - val_precision: 0.9595 - val_recall: 0.9775 - learning_rate: 0.0031\n",
      "Epoch 17/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9756 - f1_score: 0.6759 - loss: 0.1108 - precision: 0.9747 - recall: 0.9771 - val_accuracy: 0.9712 - val_f1_score: 0.6675 - val_loss: 0.1241 - val_precision: 0.9620 - val_recall: 0.9812 - learning_rate: 0.0031\n",
      "Epoch 18/20\n",
      "\u001b[1m191/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9762 - f1_score: 0.6753 - loss: 0.1068 - precision: 0.9739 - recall: 0.9794"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9762 - f1_score: 0.6752 - loss: 0.1069 - precision: 0.9738 - recall: 0.9793 - val_accuracy: 0.9725 - val_f1_score: 0.6723 - val_loss: 0.1179 - val_precision: 0.9644 - val_recall: 0.9812 - learning_rate: 0.0031\n",
      "Epoch 19/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9804 - f1_score: 0.6832 - loss: 0.1008 - precision: 0.9794 - recall: 0.9820 - val_accuracy: 0.9706 - val_f1_score: 0.6720 - val_loss: 0.1325 - val_precision: 0.9608 - val_recall: 0.9812 - learning_rate: 0.0031\n",
      "Epoch 20/20\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9804 - f1_score: 0.6804 - loss: 0.0968 - precision: 0.9783 - recall: 0.9830 - val_accuracy: 0.9631 - val_f1_score: 0.6683 - val_loss: 0.1619 - val_precision: 0.9458 - val_recall: 0.9825 - learning_rate: 0.0031\n",
      "\n",
      "============================================================\n",
      "EVALUACIÓN FINAL EN CONJUNTO DE PRUEBA\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEVALUACIÓN FINAL EN CONJUNTO DE PRUEBA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_comprehensive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMétricas principales:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 194\u001b[0m, in \u001b[0;36mPhishingNeuralNetwork.evaluate_comprehensive\u001b[0;34m(self, X_test, y_test)\u001b[0m\n\u001b[1;32m    191\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model\u001b[38;5;241m.\u001b[39mpredict(X_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    192\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m (y_pred_proba \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m--> 194\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m(y_test, y_pred)\n\u001b[1;32m    195\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(y_test, y_pred)\n\u001b[1;32m    196\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(y_test, y_pred)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "# Inicializar el modelo\n",
    "nn_model = PhishingNeuralNetwork(random_state=42)\n",
    "\n",
    "# Preparar los datos según metodología (70%-15%-15%)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = nn_model.prepare_data(X, y)\n",
    "\n",
    "# Optimización de hiperparámetros\n",
    "best_hyperparams = nn_model.optimize_hyperparameters(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    max_trials=20,  # Reducido para demo, usar 50+ en producción\n",
    "    epochs=20       # Reducido para demo, usar 100+ en producción\n",
    ")\n",
    "\n",
    "# Entrenar modelo final con mejores hiperparámetros\n",
    "final_model = nn_model.train_final_model(\n",
    "    X_train, y_train, X_val, y_val, \n",
    "    best_hyperparams, epochs=20\n",
    ")\n",
    "\n",
    "# Evaluación final en conjunto de prueba\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUACIÓN FINAL EN CONJUNTO DE PRUEBA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_metrics = nn_model.evaluate_comprehensive(X_test, y_test)\n",
    "\n",
    "print(f\"\\nMétricas principales:\")\n",
    "print(f\"Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {test_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"AUC-ROC: {test_metrics['auc_roc']:.4f}\")\n",
    "\n",
    "print(f\"\\nMétricas complementarias:\")\n",
    "print(f\"Specificity: {test_metrics['specificity']:.4f}\")\n",
    "print(f\"Matthews Correlation Coefficient: {test_metrics['mcc']:.4f}\")\n",
    "\n",
    "# Verificar criterios de selección según metodología\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFICACIÓN DE CRITERIOS DE SELECCIÓN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"✓ F1-Score maximizado: {test_metrics['f1_score']:.4f}\")\n",
    "print(f\"{'✓' if test_metrics['auc_roc'] >= 0.90 else '✗'} AUC-ROC ≥ 0.90: {test_metrics['auc_roc']:.4f}\")\n",
    "\n",
    "# Visualizaciones\n",
    "nn_model.plot_comprehensive_results(test_metrics)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"IMPLEMENTACIÓN COMPLETADA SEGÚN METODOLOGÍA\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

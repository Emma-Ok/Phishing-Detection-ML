{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeQWosDCVIRwtAmJz4Nkd9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Emma-Ok/Phishing-Detection-ML/blob/main/featureSelectionAlgorithms.py\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dg64Ynb_bJyK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "NBRtiE9NU8Ne"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification, load_iris, load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import Lasso, LassoCV\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "class FeatureSelectionAlgorithms:\n",
        "    \"\"\"\n",
        "    Clase que implementa algoritmos de selección y extracción de características\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.scaler = StandardScaler()\n",
        "        self.selected_features = None\n",
        "\n",
        "    def lasso_feature_selection(self, X, y, alpha=None, plot=True):\n",
        "        \"\"\"\n",
        "        Selección de características usando LASSO\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Matriz de características\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Vector objetivo\n",
        "        alpha : float, opcional\n",
        "            Parámetro de regularización. Si None, se usa validación cruzada\n",
        "        plot : bool\n",
        "            Si mostrar gráficos de los coeficientes\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        X_selected : array-like\n",
        "            Características seleccionadas\n",
        "        selected_indices : array\n",
        "            Índices de las características seleccionadas\n",
        "        \"\"\"\n",
        "        print(\"=== SELECCIÓN DE CARACTERÍSTICAS CON LASSO ===\")\n",
        "\n",
        "        # Normalizar datos\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        if alpha is None:\n",
        "            # Usar validación cruzada para encontrar el mejor alpha\n",
        "            lasso_cv = LassoCV(cv=5, random_state=42, max_iter=2000)\n",
        "            lasso_cv.fit(X_scaled, y)\n",
        "            alpha = lasso_cv.alpha_\n",
        "            print(f\"Alpha óptimo encontrado: {alpha:.4f}\")\n",
        "\n",
        "        # Aplicar LASSO\n",
        "        lasso = Lasso(alpha=alpha, max_iter=2000)\n",
        "        lasso.fit(X_scaled, y)\n",
        "\n",
        "        # Obtener características seleccionadas (coeficientes != 0)\n",
        "        selected_indices = np.where(lasso.coef_ != 0)[0]\n",
        "        X_selected = X_scaled[:, selected_indices]\n",
        "\n",
        "        print(f\"Características originales: {X.shape[1]}\")\n",
        "        print(f\"Características seleccionadas: {len(selected_indices)}\")\n",
        "        print(f\"Índices seleccionados: {selected_indices}\")\n",
        "\n",
        "        if plot and len(selected_indices) > 0:\n",
        "            plt.figure(figsize=(12, 6))\n",
        "\n",
        "            # Gráfico de coeficientes\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.bar(range(len(lasso.coef_)), lasso.coef_)\n",
        "            plt.title('Coeficientes LASSO')\n",
        "            plt.xlabel('Índice de característica')\n",
        "            plt.ylabel('Coeficiente')\n",
        "            plt.axhline(y=0, color='r', linestyle='--', alpha=0.7)\n",
        "\n",
        "            # Path de regularización\n",
        "            plt.subplot(1, 2, 2)\n",
        "            alphas = np.logspace(-4, 1, 50)\n",
        "            coefs = []\n",
        "            for a in alphas:\n",
        "                lasso_temp = Lasso(alpha=a, max_iter=2000)\n",
        "                lasso_temp.fit(X_scaled, y)\n",
        "                coefs.append(lasso_temp.coef_)\n",
        "\n",
        "            coefs = np.array(coefs)\n",
        "            for i in range(coefs.shape[1]):\n",
        "                plt.plot(alphas, coefs[:, i], alpha=0.7)\n",
        "            plt.xscale('log')\n",
        "            plt.xlabel('Alpha')\n",
        "            plt.ylabel('Coeficientes')\n",
        "            plt.title('Path de Regularización LASSO')\n",
        "            plt.axvline(x=alpha, color='r', linestyle='--', label=f'Alpha óptimo: {alpha:.4f}')\n",
        "            plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        self.selected_features = selected_indices\n",
        "        return X_selected, selected_indices\n",
        "\n",
        "    def pca_feature_extraction(self, X, n_components=None, variance_threshold=0.95, plot=True):\n",
        "        \"\"\"\n",
        "        Extracción de características usando PCA\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Matriz de características\n",
        "        n_components : int, opcional\n",
        "            Número de componentes. Si None, se determina por variance_threshold\n",
        "        variance_threshold : float\n",
        "            Umbral de varianza explicada acumulada\n",
        "        plot : bool\n",
        "            Si mostrar gráficos de PCA\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        X_pca : array-like\n",
        "            Datos transformados por PCA\n",
        "        pca : PCA object\n",
        "            Objeto PCA ajustado\n",
        "        \"\"\"\n",
        "        print(\"\\n=== EXTRACCIÓN DE CARACTERÍSTICAS CON PCA ===\")\n",
        "\n",
        "        # Normalizar datos\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Determinar número de componentes\n",
        "        if n_components is None:\n",
        "            pca_temp = PCA()\n",
        "            pca_temp.fit(X_scaled)\n",
        "            cumsum_var = np.cumsum(pca_temp.explained_variance_ratio_)\n",
        "            n_components = np.argmax(cumsum_var >= variance_threshold) + 1\n",
        "\n",
        "        # Aplicar PCA\n",
        "        pca = PCA(n_components=n_components)\n",
        "        X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "        print(f\"Características originales: {X.shape[1]}\")\n",
        "        print(f\"Componentes principales: {n_components}\")\n",
        "        print(f\"Varianza explicada por componente: {pca.explained_variance_ratio_}\")\n",
        "        print(f\"Varianza explicada total: {pca.explained_variance_ratio_.sum():.4f}\")\n",
        "\n",
        "        if plot:\n",
        "            plt.figure(figsize=(15, 10))\n",
        "\n",
        "            # Varianza explicada por componente\n",
        "            plt.subplot(2, 3, 1)\n",
        "            plt.bar(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "                   pca.explained_variance_ratio_)\n",
        "            plt.title('Varianza Explicada por Componente')\n",
        "            plt.xlabel('Componente Principal')\n",
        "            plt.ylabel('Proporción de Varianza')\n",
        "\n",
        "            # Varianza explicada acumulada\n",
        "            plt.subplot(2, 3, 2)\n",
        "            plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
        "                    np.cumsum(pca.explained_variance_ratio_), 'bo-')\n",
        "            plt.axhline(y=variance_threshold, color='r', linestyle='--',\n",
        "                       label=f'Umbral: {variance_threshold}')\n",
        "            plt.title('Varianza Explicada Acumulada')\n",
        "            plt.xlabel('Número de Componentes')\n",
        "            plt.ylabel('Varianza Acumulada')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Heatmap de componentes principales (si hay suficientes features)\n",
        "            if X.shape[1] <= 20:\n",
        "                plt.subplot(2, 3, 3)\n",
        "                sns.heatmap(pca.components_, cmap='RdBu_r', center=0,\n",
        "                           xticklabels=range(X.shape[1]),\n",
        "                           yticklabels=[f'PC{i+1}' for i in range(n_components)])\n",
        "                plt.title('Componentes Principales\\n(Pesos de características)')\n",
        "                plt.xlabel('Característica Original')\n",
        "                plt.ylabel('Componente Principal')\n",
        "\n",
        "            # Proyección en las primeras dos componentes (si es posible)\n",
        "            if n_components >= 2:\n",
        "                plt.subplot(2, 3, 4)\n",
        "                plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.7)\n",
        "                plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')\n",
        "                plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')\n",
        "                plt.title('Proyección en PC1 vs PC2')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Biplot (si hay pocas características)\n",
        "            if X.shape[1] <= 10 and n_components >= 2:\n",
        "                plt.subplot(2, 3, 5)\n",
        "                # Puntos de datos\n",
        "                plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\n",
        "\n",
        "                # Vectores de características\n",
        "                feature_vectors = pca.components_[:2].T * np.sqrt(pca.explained_variance_[:2])\n",
        "                for i, (x, y) in enumerate(feature_vectors):\n",
        "                    plt.arrow(0, 0, x, y, head_width=0.1, head_length=0.1,\n",
        "                             fc='red', ec='red', alpha=0.7)\n",
        "                    plt.text(x*1.1, y*1.1, f'F{i}', fontsize=10, ha='center', va='center')\n",
        "\n",
        "                plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.3f})')\n",
        "                plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.3f})')\n",
        "                plt.title('Biplot PCA')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        return X_pca, pca\n",
        "\n",
        "    def lda_feature_extraction(self, X, y, plot=True):\n",
        "        \"\"\"\n",
        "        Extracción de características usando LDA (Linear Discriminant Analysis)\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Matriz de características\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Vector de etiquetas\n",
        "        plot : bool\n",
        "            Si mostrar gráficos de LDA\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        X_lda : array-like\n",
        "            Datos transformados por LDA\n",
        "        lda : LDA object\n",
        "            Objeto LDA ajustado\n",
        "        \"\"\"\n",
        "        print(\"\\n=== EXTRACCIÓN DE CARACTERÍSTICAS CON LDA ===\")\n",
        "\n",
        "        # Normalizar datos\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Determinar número máximo de componentes\n",
        "        n_classes = len(np.unique(y))\n",
        "        n_features = X.shape[1]\n",
        "        max_components = min(n_classes - 1, n_features)\n",
        "\n",
        "        # Aplicar LDA\n",
        "        lda = LDA(n_components=max_components)\n",
        "        X_lda = lda.fit_transform(X_scaled, y)\n",
        "\n",
        "        print(f\"Características originales: {X.shape[1]}\")\n",
        "        print(f\"Número de clases: {n_classes}\")\n",
        "        print(f\"Componentes LDA: {X_lda.shape[1]}\")\n",
        "        print(f\"Proporción de varianza explicada: {lda.explained_variance_ratio_}\")\n",
        "\n",
        "        if plot:\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            # Varianza explicada por componente\n",
        "            plt.subplot(1, 3, 1)\n",
        "            components = range(1, len(lda.explained_variance_ratio_) + 1)\n",
        "            plt.bar(components, lda.explained_variance_ratio_)\n",
        "            plt.title('Varianza Explicada por Componente LDA')\n",
        "            plt.xlabel('Componente Discriminante')\n",
        "            plt.ylabel('Proporción de Varianza')\n",
        "\n",
        "            # Proyección en el primer componente (si existe)\n",
        "            if X_lda.shape[1] >= 1:\n",
        "                plt.subplot(1, 3, 2)\n",
        "                for class_label in np.unique(y):\n",
        "                    mask = y == class_label\n",
        "                    plt.hist(X_lda[mask, 0], alpha=0.7, label=f'Clase {class_label}')\n",
        "                plt.xlabel('LD1')\n",
        "                plt.ylabel('Frecuencia')\n",
        "                plt.title('Distribución en LD1')\n",
        "                plt.legend()\n",
        "\n",
        "            # Proyección en las primeras dos componentes (si existen)\n",
        "            if X_lda.shape[1] >= 2:\n",
        "                plt.subplot(1, 3, 3)\n",
        "                for class_label in np.unique(y):\n",
        "                    mask = y == class_label\n",
        "                    plt.scatter(X_lda[mask, 0], X_lda[mask, 1],\n",
        "                              label=f'Clase {class_label}', alpha=0.7)\n",
        "                plt.xlabel(f'LD1 ({lda.explained_variance_ratio_[0]:.3f})')\n",
        "                plt.ylabel(f'LD2 ({lda.explained_variance_ratio_[1]:.3f})')\n",
        "                plt.title('Proyección en LD1 vs LD2')\n",
        "                plt.legend()\n",
        "                plt.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        return X_lda, lda\n",
        "\n",
        "    def fisher_discriminant(self, X, y, plot=True):\n",
        "        \"\"\"\n",
        "        Implementación del discriminante de Fisher\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like, shape (n_samples, n_features)\n",
        "            Matriz de características\n",
        "        y : array-like, shape (n_samples,)\n",
        "            Vector de etiquetas (debe ser binario)\n",
        "        plot : bool\n",
        "            Si mostrar gráficos\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        w : array\n",
        "            Vector de pesos del discriminante de Fisher\n",
        "        X_fisher : array\n",
        "            Proyección de los datos en la dirección de Fisher\n",
        "        \"\"\"\n",
        "        print(\"\\n=== DISCRIMINANTE DE FISHER ===\")\n",
        "\n",
        "        # Verificar que sea un problema de clasificación binaria\n",
        "        classes = np.unique(y)\n",
        "        if len(classes) != 2:\n",
        "            raise ValueError(\"El discriminante de Fisher está implementado solo para clasificación binaria\")\n",
        "\n",
        "        # Normalizar datos\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        # Separar clases\n",
        "        class1_mask = y == classes[0]\n",
        "        class2_mask = y == classes[1]\n",
        "\n",
        "        X1 = X_scaled[class1_mask]\n",
        "        X2 = X_scaled[class2_mask]\n",
        "\n",
        "        # Calcular medias de cada clase\n",
        "        mu1 = np.mean(X1, axis=0)\n",
        "        mu2 = np.mean(X2, axis=0)\n",
        "\n",
        "        # Calcular matrices de covarianza intra-clase\n",
        "        S1 = np.cov(X1.T)\n",
        "        S2 = np.cov(X2.T)\n",
        "\n",
        "        # Matriz de covarianza intra-clase total\n",
        "        Sw = S1 + S2\n",
        "\n",
        "        # Vector de diferencia de medias\n",
        "        mean_diff = mu2 - mu1\n",
        "\n",
        "        # Calcular el vector de pesos de Fisher\n",
        "        try:\n",
        "            w = np.linalg.solve(Sw, mean_diff)\n",
        "        except np.linalg.LinAlgError:\n",
        "            # Si Sw no es invertible, usar pseudoinversa\n",
        "            w = np.linalg.pinv(Sw) @ mean_diff\n",
        "\n",
        "        # Normalizar el vector de pesos\n",
        "        w = w / np.linalg.norm(w)\n",
        "\n",
        "        # Proyectar los datos\n",
        "        X_fisher = X_scaled @ w\n",
        "\n",
        "        # Calcular métricas de separabilidad\n",
        "        proj1 = X_fisher[class1_mask]\n",
        "        proj2 = X_fisher[class2_mask]\n",
        "\n",
        "        between_class_var = (np.mean(proj1) - np.mean(proj2))**2\n",
        "        within_class_var = np.var(proj1) + np.var(proj2)\n",
        "        fisher_ratio = between_class_var / within_class_var if within_class_var > 0 else 0\n",
        "\n",
        "        print(f\"Clases: {classes}\")\n",
        "        print(f\"Tamaño clase {classes[0]}: {len(X1)}\")\n",
        "        print(f\"Tamaño clase {classes[1]}: {len(X2)}\")\n",
        "        print(f\"Ratio de Fisher: {fisher_ratio:.4f}\")\n",
        "        print(f\"Vector de pesos shape: {w.shape}\")\n",
        "\n",
        "        if plot:\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            # Histograma de proyecciones\n",
        "            plt.subplot(1, 3, 1)\n",
        "            plt.hist(proj1, alpha=0.7, bins=20, label=f'Clase {classes[0]}')\n",
        "            plt.hist(proj2, alpha=0.7, bins=20, label=f'Clase {classes[1]}')\n",
        "            plt.xlabel('Proyección de Fisher')\n",
        "            plt.ylabel('Frecuencia')\n",
        "            plt.title('Distribución de Proyecciones')\n",
        "            plt.legend()\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Vector de pesos\n",
        "            plt.subplot(1, 3, 2)\n",
        "            plt.bar(range(len(w)), w)\n",
        "            plt.title('Vector de Pesos de Fisher')\n",
        "            plt.xlabel('Índice de Característica')\n",
        "            plt.ylabel('Peso')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "\n",
        "            # Proyección 2D (si hay al menos 2 características)\n",
        "            if X.shape[1] >= 2:\n",
        "                plt.subplot(1, 3, 3)\n",
        "                plt.scatter(X_scaled[class1_mask, 0], X_scaled[class1_mask, 1],\n",
        "                           alpha=0.7, label=f'Clase {classes[0]}')\n",
        "                plt.scatter(X_scaled[class2_mask, 0], X_scaled[class2_mask, 1],\n",
        "                           alpha=0.7, label=f'Clase {classes[1]}')\n",
        "\n",
        "                # Dibujar la dirección de Fisher\n",
        "                center = np.mean(X_scaled, axis=0)\n",
        "                direction = w[:2] if len(w) >= 2 else np.array([w[0], 0])\n",
        "                plt.arrow(center[0], center[1], direction[0], direction[1],\n",
        "                         head_width=0.1, head_length=0.1, fc='red', ec='red',\n",
        "                         linewidth=2, label='Dirección Fisher')\n",
        "\n",
        "                plt.xlabel('Característica 1')\n",
        "                plt.ylabel('Característica 2')\n",
        "                plt.title('Datos y Dirección de Fisher')\n",
        "                plt.legend()\n",
        "                plt.grid(True, alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        return w, X_fisher\n",
        "\n",
        "    def compare_methods(self, X, y, test_size=0.3, focus_on_recall=True):\n",
        "        \"\"\"\n",
        "        Comparar todos los métodos con métricas detalladas enfocadas en minimizar falsos negativos\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : array-like\n",
        "            Matriz de características\n",
        "        y : array-like\n",
        "            Vector objetivo\n",
        "        test_size : float\n",
        "            Proporción de datos para test\n",
        "        focus_on_recall : bool\n",
        "            Si enfocar en recall (minimizar falsos negativos)\n",
        "        \"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"COMPARACIÓN DETALLADA DE MÉTODOS\")\n",
        "        print(\"OBJETIVO: MINIMIZAR FALSOS NEGATIVOS (MAXIMIZAR RECALL)\")\n",
        "        print(\"=\"*70)\n",
        "\n",
        "        from sklearn.metrics import (precision_score, recall_score, f1_score,\n",
        "                                   confusion_matrix, roc_auc_score,\n",
        "                                   precision_recall_curve, roc_curve)\n",
        "        from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "        # Dividir datos\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,\n",
        "                                                           random_state=42, stratify=y)\n",
        "\n",
        "        # Escalar datos\n",
        "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
        "        X_test_scaled = self.scaler.transform(X_test)\n",
        "\n",
        "        results = {}\n",
        "        predictions = {}\n",
        "        probabilities = {}\n",
        "\n",
        "        # Datos originales\n",
        "        clf_original = LogisticRegression(random_state=42, max_iter=1000)\n",
        "        clf_original.fit(X_train_scaled, y_train)\n",
        "        y_pred_original = clf_original.predict(X_test_scaled)\n",
        "        y_proba_original = clf_original.predict_proba(X_test_scaled)[:, 1] if len(np.unique(y)) == 2 else None\n",
        "\n",
        "        predictions['Original'] = y_pred_original\n",
        "        probabilities['Original'] = y_proba_original\n",
        "\n",
        "        # LASSO\n",
        "        try:\n",
        "            X_train_lasso, selected_indices = self.lasso_feature_selection(X_train, y_train, plot=False)\n",
        "            if len(selected_indices) > 0:\n",
        "                X_test_lasso = self.scaler.transform(X_test)[:, selected_indices]\n",
        "                clf_lasso = LogisticRegression(random_state=42, max_iter=1000)\n",
        "                clf_lasso.fit(X_train_lasso, y_train)\n",
        "                y_pred_lasso = clf_lasso.predict(X_test_lasso)\n",
        "                y_proba_lasso = clf_lasso.predict_proba(X_test_lasso)[:, 1] if len(np.unique(y)) == 2 else None\n",
        "                predictions['LASSO'] = y_pred_lasso\n",
        "                probabilities['LASSO'] = y_proba_lasso\n",
        "            else:\n",
        "                predictions['LASSO'] = np.zeros_like(y_test)\n",
        "                probabilities['LASSO'] = None\n",
        "        except Exception as e:\n",
        "            print(f\"Error en LASSO: {e}\")\n",
        "            predictions['LASSO'] = np.zeros_like(y_test)\n",
        "            probabilities['LASSO'] = None\n",
        "\n",
        "        # PCA\n",
        "        try:\n",
        "            X_train_pca, pca = self.pca_feature_extraction(X_train, plot=False)\n",
        "            X_test_pca = pca.transform(self.scaler.transform(X_test))\n",
        "            clf_pca = LogisticRegression(random_state=42, max_iter=1000)\n",
        "            clf_pca.fit(X_train_pca, y_train)\n",
        "            y_pred_pca = clf_pca.predict(X_test_pca)\n",
        "            y_proba_pca = clf_pca.predict_proba(X_test_pca)[:, 1] if len(np.unique(y)) == 2 else None\n",
        "            predictions['PCA'] = y_pred_pca\n",
        "            probabilities['PCA'] = y_proba_pca\n",
        "        except Exception as e:\n",
        "            print(f\"Error en PCA: {e}\")\n",
        "            predictions['PCA'] = np.zeros_like(y_test)\n",
        "            probabilities['PCA'] = None\n",
        "\n",
        "        # LDA\n",
        "        try:\n",
        "            X_train_lda, lda = self.lda_feature_extraction(X_train, y_train, plot=False)\n",
        "            X_test_lda = lda.transform(self.scaler.transform(X_test))\n",
        "            clf_lda = LogisticRegression(random_state=42, max_iter=1000)\n",
        "            clf_lda.fit(X_train_lda, y_train)\n",
        "            y_pred_lda = clf_lda.predict(X_test_lda)\n",
        "            y_proba_lda = clf_lda.predict_proba(X_test_lda)[:, 1] if len(np.unique(y)) == 2 else None\n",
        "            predictions['LDA'] = y_pred_lda\n",
        "            probabilities['LDA'] = y_proba_lda\n",
        "        except Exception as e:\n",
        "            print(f\"Error en LDA: {e}\")\n",
        "            predictions['LDA'] = np.zeros_like(y_test)\n",
        "            probabilities['LDA'] = None\n",
        "\n",
        "        # Fisher (solo para clasificación binaria)\n",
        "        if len(np.unique(y)) == 2:\n",
        "            try:\n",
        "                w, _ = self.fisher_discriminant(X_train, y_train, plot=False)\n",
        "                X_train_fisher = (self.scaler.fit_transform(X_train) @ w).reshape(-1, 1)\n",
        "                X_test_fisher = (self.scaler.transform(X_test) @ w).reshape(-1, 1)\n",
        "                clf_fisher = LogisticRegression(random_state=42, max_iter=1000)\n",
        "                clf_fisher.fit(X_train_fisher, y_train)\n",
        "                y_pred_fisher = clf_fisher.predict(X_test_fisher)\n",
        "                y_proba_fisher = clf_fisher.predict_proba(X_test_fisher)[:, 1]\n",
        "                predictions['Fisher'] = y_pred_fisher\n",
        "                probabilities['Fisher'] = y_proba_fisher\n",
        "            except Exception as e:\n",
        "                print(f\"Error en Fisher: {e}\")\n",
        "                predictions['Fisher'] = np.zeros_like(y_test)\n",
        "                probabilities['Fisher'] = None\n",
        "\n",
        "        # Calcular métricas para cada método\n",
        "        for method in predictions.keys():\n",
        "            y_pred = predictions[method]\n",
        "\n",
        "            # Métricas básicas\n",
        "            accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "            # Para problemas multiclase, usar promedio macro\n",
        "            if len(np.unique(y)) > 2:\n",
        "                precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "            else:\n",
        "                precision = precision_score(y_test, y_pred, zero_division=0)\n",
        "                recall = recall_score(y_test, y_pred, zero_division=0)\n",
        "                f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "\n",
        "            # Matriz de confusión\n",
        "            cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "            # Para clasificación binaria, calcular falsos negativos y AUC\n",
        "            if len(np.unique(y)) == 2:\n",
        "                tn, fp, fn, tp = cm.ravel()\n",
        "                fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # Tasa de falsos positivos\n",
        "                fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # Tasa de falsos negativos\n",
        "\n",
        "                # AUC-ROC si tenemos probabilidades\n",
        "                auc_roc = None\n",
        "                if probabilities[method] is not None:\n",
        "                    try:\n",
        "                        auc_roc = roc_auc_score(y_test, probabilities[method])\n",
        "                    except:\n",
        "                        auc_roc = None\n",
        "\n",
        "                results[method] = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'false_negatives': fn,\n",
        "                    'false_positives': fp,\n",
        "                    'true_positives': tp,\n",
        "                    'true_negatives': tn,\n",
        "                    'false_negative_rate': fnr,\n",
        "                    'false_positive_rate': fpr,\n",
        "                    'auc_roc': auc_roc,\n",
        "                    'confusion_matrix': cm\n",
        "                }\n",
        "            else:\n",
        "                # Para multiclase\n",
        "                total_fn = 0\n",
        "                total_fp = 0\n",
        "                for i in range(len(cm)):\n",
        "                    total_fn += cm[:, i].sum() - cm[i, i]  # Suma de falsos negativos\n",
        "                    total_fp += cm[i, :].sum() - cm[i, i]  # Suma de falsos positivos\n",
        "\n",
        "                results[method] = {\n",
        "                    'accuracy': accuracy,\n",
        "                    'precision': precision,\n",
        "                    'recall': recall,\n",
        "                    'f1': f1,\n",
        "                    'total_false_negatives': total_fn,\n",
        "                    'total_false_positives': total_fp,\n",
        "                    'confusion_matrix': cm\n",
        "                }\n",
        "\n",
        "        # Mostrar resultados tabulares\n",
        "        print(\"\\nRESULTADOS DETALLADOS:\")\n",
        "        print(\"=\" * 90)\n",
        "\n",
        "        if len(np.unique(y)) == 2:\n",
        "            # Clasificación binaria\n",
        "            header = f\"{'Método':<12} {'Accuracy':<9} {'Precision':<10} {'Recall':<8} {'F1':<8} {'FN':<4} {'FP':<4} {'FNR':<8} {'AUC':<8}\"\n",
        "            print(header)\n",
        "            print(\"-\" * len(header))\n",
        "\n",
        "            for method, metrics in results.items():\n",
        "                auc_str = f\"{metrics['auc_roc']:.3f}\" if metrics['auc_roc'] is not None else \"N/A\"\n",
        "                print(f\"{method:<12} {metrics['accuracy']:<9.3f} {metrics['precision']:<10.3f} \"\n",
        "                      f\"{metrics['recall']:<8.3f} {metrics['f1']:<8.3f} {metrics['false_negatives']:<4} \"\n",
        "                      f\"{metrics['false_positives']:<4} {metrics['false_negative_rate']:<8.3f} {auc_str:<8}\")\n",
        "        else:\n",
        "            # Clasificación multiclase\n",
        "            header = f\"{'Método':<12} {'Accuracy':<9} {'Precision':<10} {'Recall':<8} {'F1':<8} {'Total FN':<9}\"\n",
        "            print(header)\n",
        "            print(\"-\" * len(header))\n",
        "\n",
        "            for method, metrics in results.items():\n",
        "                print(f\"{method:<12} {metrics['accuracy']:<9.3f} {metrics['precision']:<10.3f} \"\n",
        "                      f\"{metrics['recall']:<8.3f} {metrics['f1']:<8.3f} {metrics['total_false_negatives']:<9}\")\n",
        "\n",
        "        # Análisis de mejor método\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(\"ANÁLISIS PARA MINIMIZAR FALSOS NEGATIVOS:\")\n",
        "        print(f\"{'='*50}\")\n",
        "\n",
        "        if len(np.unique(y)) == 2:\n",
        "            # Ordenar por menor tasa de falsos negativos (mayor recall)\n",
        "            sorted_methods = sorted(results.items(), key=lambda x: x[1]['recall'], reverse=True)\n",
        "\n",
        "            print(f\"\\nRanking por RECALL (menor falsos negativos):\")\n",
        "            for i, (method, metrics) in enumerate(sorted_methods, 1):\n",
        "                print(f\"{i}. {method}: Recall={metrics['recall']:.3f}, \"\n",
        "                      f\"FN={metrics['false_negatives']}, FNR={metrics['false_negative_rate']:.3f}\")\n",
        "\n",
        "            best_method = sorted_methods[0][0]\n",
        "            best_metrics = sorted_methods[0][1]\n",
        "\n",
        "            print(f\"\\n🏆 MEJOR MÉTODO: {best_method}\")\n",
        "            print(f\"   • Recall: {best_metrics['recall']:.3f}\")\n",
        "            print(f\"   • Falsos Negativos: {best_metrics['false_negatives']}\")\n",
        "            print(f\"   • Tasa de Falsos Negativos: {best_metrics['false_negative_rate']:.3f}\")\n",
        "            print(f\"   • F1-Score: {best_metrics['f1']:.3f}\")\n",
        "\n",
        "        # Visualizaciones\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('Comparación Detallada de Métodos - Enfoque en Minimizar Falsos Negativos',\n",
        "                     fontsize=16, y=0.98)\n",
        "\n",
        "        methods = list(results.keys())\n",
        "\n",
        "        # 1. Recall (principal métrica)\n",
        "        recalls = [results[m]['recall'] for m in methods]\n",
        "        bars1 = axes[0, 0].bar(methods, recalls, color='lightgreen', alpha=0.8)\n",
        "        axes[0, 0].set_title('Recall (Sensibilidad)\\n¡MAYOR ES MEJOR!', fontweight='bold')\n",
        "        axes[0, 0].set_ylabel('Recall')\n",
        "        axes[0, 0].set_ylim(0, 1)\n",
        "        for bar, val in zip(bars1, recalls):\n",
        "            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                           f'{val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        # 2. Falsos Negativos (absolutos)\n",
        "        if len(np.unique(y)) == 2:\n",
        "            fns = [results[m]['false_negatives'] for m in methods]\n",
        "            bars2 = axes[0, 1].bar(methods, fns, color='lightcoral', alpha=0.8)\n",
        "            axes[0, 1].set_title('Falsos Negativos\\n¡MENOR ES MEJOR!', fontweight='bold')\n",
        "            axes[0, 1].set_ylabel('Número de FN')\n",
        "            for bar, val in zip(bars2, fns):\n",
        "                axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
        "                               f'{val}', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "        # 3. F1-Score\n",
        "        f1s = [results[m]['f1'] for m in methods]\n",
        "        bars3 = axes[0, 2].bar(methods, f1s, color='gold', alpha=0.8)\n",
        "        axes[0, 2].set_title('F1-Score\\n(Balance Precision-Recall)', fontweight='bold')\n",
        "        axes[0, 2].set_ylabel('F1-Score')\n",
        "        axes[0, 2].set_ylim(0, 1)\n",
        "        for bar, val in zip(bars3, f1s):\n",
        "            axes[0, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                           f'{val:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # 4. Accuracy\n",
        "        accuracies = [results[m]['accuracy'] for m in methods]\n",
        "        bars4 = axes[1, 0].bar(methods, accuracies, color='skyblue', alpha=0.8)\n",
        "        axes[1, 0].set_title('Accuracy', fontweight='bold')\n",
        "        axes[1, 0].set_ylabel('Accuracy')\n",
        "        axes[1, 0].set_ylim(0, 1)\n",
        "        for bar, val in zip(bars4, accuracies):\n",
        "            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                           f'{val:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # 5. Precision\n",
        "        precisions = [results[m]['precision'] for m in methods]\n",
        "        bars5 = axes[1, 1].bar(methods, precisions, color='plum', alpha=0.8)\n",
        "        axes[1, 1].set_title('Precision\\n(de predicciones positivas)', fontweight='bold')\n",
        "        axes[1, 1].set_ylabel('Precision')\n",
        "        axes[1, 1].set_ylim(0, 1)\n",
        "        for bar, val in zip(bars5, precisions):\n",
        "            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                           f'{val:.3f}', ha='center', va='bottom')\n",
        "\n",
        "        # 6. Comparación Recall vs Precision\n",
        "        axes[1, 2].scatter(precisions, recalls, s=100, alpha=0.7)\n",
        "        for i, method in enumerate(methods):\n",
        "            axes[1, 2].annotate(method, (precisions[i], recalls[i]),\n",
        "                               xytext=(5, 5), textcoords='offset points')\n",
        "        axes[1, 2].set_xlabel('Precision')\n",
        "        axes[1, 2].set_ylabel('Recall')\n",
        "        axes[1, 2].set_title('Precision vs Recall\\n(Esquina superior derecha es ideal)')\n",
        "        axes[1, 2].grid(True, alpha=0.3)\n",
        "        axes[1, 2].set_xlim(0, 1)\n",
        "        axes[1, 2].set_ylim(0, 1)\n",
        "\n",
        "        # Rotar etiquetas del eje x\n",
        "        for ax in axes.flat:\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Mostrar matrices de confusión para clasificación binaria\n",
        "        if len(np.unique(y)) == 2:\n",
        "            n_methods = len(methods)\n",
        "            fig, axes = plt.subplots(1, n_methods, figsize=(4*n_methods, 4))\n",
        "            if n_methods == 1:\n",
        "                axes = [axes]\n",
        "\n",
        "            fig.suptitle('Matrices de Confusión - Enfoque en minimizar FN (esquina inferior izquierda)',\n",
        "                         fontsize=14, y=1.02)\n",
        "\n",
        "            for i, method in enumerate(methods):\n",
        "                cm = results[method]['confusion_matrix']\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
        "                           xticklabels=['Pred: Neg', 'Pred: Pos'],\n",
        "                           yticklabels=['Real: Neg', 'Real: Pos'])\n",
        "                axes[i].set_title(f'{method}\\nFN: {results[method][\"false_negatives\"]}')\n",
        "\n",
        "                # Resaltar falsos negativos\n",
        "                axes[i].add_patch(plt.Rectangle((0, 1), 1, 1, fill=False,\n",
        "                                              edgecolor='red', lw=3))\n",
        "                axes[i].text(0.5, 1.5, 'FN!', ha='center', va='center',\n",
        "                           color='red', fontweight='bold', fontsize=12)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "        return results\n",
        "\n"
      ]
    }
  ]
}